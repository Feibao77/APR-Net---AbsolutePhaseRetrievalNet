# APR-Net---AbsolutePhaseRetrievalNet
If you need more information, see our [paper]().

## Introduction
APR-Net enables pixel-wise prediction of the absolute phase map from a single deformed frequency-multiplexing composite image. A nested strategy leveraging Residual U-block (RSU) and the concept of centralized information interaction (CII) are employed.

## Architecture of APR-Net
The backbone of APR-Net adopts two levels of U-shape structure (see Fig. 1). On the exterior level, the U-shape structure consists of several encoder modules and decoder modules. On the interior level, each exterior encoder or decoder is a RSU (see Fig. 2) which is also a U-shape structure. The nested U-shape structure facilitates hierarchical feature extraction, allowing the network to capture multi-scale information inherent in the input. 

![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/59141cb3-2b73-4161-a73c-8692bc1e3d5c)

**Fig.1.** Schematic of APR-Net.


![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/1ea2698c-251b-4d06-b378-f9c250ab46f2)

**Fig. 2.** Schematic of RSU. (a) RSU-L. (b) RSU-4F.


Additionally, rather than using a direct identity mapping from an exterior encoder output to the corresponding decoder input, we incorporate a relative global calibration (RGC) module (see Fig. 3) into the network to strengthen the information interaction among feature maps in diverse resolutions. 
It should be noted that there is only one learnable RGC module in APR-Net, meaning the five RGC modules displayed in Fig. 1 share the same set of filter parameters. Thus, not only En_1-En_5 achieve information interaction with En_6 via RGC, but also En_1-En_5 interact with each other due to the parameters-sharing mechanism. The detailed configurations of each encoder and decoder can be seen in our paper.

![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/78f780e3-3ee5-4c9d-8e70-0cad55f14ea7)

**Fig. 3.** Architecture of RGC.


## Experiments and results
### Training
#### Pre-processing
For each measurement scenario, 49 images were captured with 640 × 640 resolution. At the start, we collected 2000 groups of samples comprising miscellaneous different scenes, such as a single sphere or several isolated plaster statues. Considering that the mapping to be learned by APR-Net might be quite complicated, we performed random cropping and horizontal flipping on the captured images and the according ground truths to enhance the diversity of our dataset. Crop sizes were 480 × 480 and 320 × 320, and flip probability was set to 0.5. The augmented 4000-sized dataset was then generated by randomly selecting 2000 groups of data from the augmented dataset and incorporating them into the original 2000-sized dataset. 
#### Loss calculation
As shown in Fig. 1, we used AP_fuse as well as AP_1-AP_6 to calculate training loss, where mean square error (MSE) was deployed. This approach could allow the network to receive feedback at multiple levels of abstraction. The formula of MSE is as follows:

![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/8d29855f-b606-4f03-b51c-64d5d8ce2c58)


After 400-epoch training that took 37 hours, bith training and validation loss converged, as shown in Fig. 4.

![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/d965ec8c-aee3-4b54-9508-e5756ebc1f29)

**Fig. 4.** Loss convergence curves. (a) Training loss convergence curve. (b) Validation loss convergence curve.


### Qualitative evaluation
Based on the calibration parameters of the fringe projection profilometry system, phase-to-height mapping was conducted to achieve 3D reconstruction. The 3D reconstruction results of DFCP were compared with those of the other four distinct methods, as shown in Fig. 5. The first row presents the 3D results by conventional tri-frequency Fourier Transform (FT) method, where the three fringe frequencies employed are identical to those in the composite pattern designed by us. The second row shows the 3D outcomes derived from the single-shot learning-based method proposed by [Nguyen et. al](https://www.sciencedirect.com/science/article/pii/S0263224121015281). In the third method, the designed APR-Net was substituted with U-Net while maintaining all other conditions unchanged. This U-Net was trained on the same dataset as APR-Net, and the predictions from this trained U-Net were used for 3D reconstruction, as shown in the third row of Fig. 5. The fourth and last rows illustrate the 3D measurement results obtained by our method and the ground-truth generation method (16-step PSP with tri-frequency TPU), respectively. Furthermore, to assess phase quality quantitively, we took ground truths as benchmarks to calculate absolute phase error maps of the other four approaches, as shown in Fig. 6. 

![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/d83ead7b-6160-49fc-9b20-c6525d8821cd)

**Fig. 5.** Comparison between the 3D reconstruction results measured by five methods. (a, b, c) The 3D results by conventional tri-frequency FT method. (d, e, f) The 3D results by Nguyen's method. (g, h, i) The 3D results by U-Net-based method. (j, k, l) The 3D results by our method. (m, n, o) The 3D results by ground-truth generation method (16-step PSP with tri-frequency TPU). 


![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/22245228-45ff-4178-9246-eda45c552f21)

**Fig. 6.** Absolute phase error maps by four methods. (a, b, c) The absolute phase error maps by conventional tri-frequency FT method. (d, e, f) The absolute phase error maps by Nguyen’s method. (g, h, i) The absolute phase error maps by U-Net-based method. (j, k, l) The absolute phase error maps by our method. 


### Quantitative evaluation
To assess the accuracy of our method quantitatively, a pair of standard spheres (Fig. 7) with calibrated diameters dA =31.7505mm, dB = 31.7391mm, and centroid distance sAB = 60.0130mm, was measured by our method. The measured values are 31.6725 mm, 31.8117 mm, and 59.9575 mm, with corresponding mean absolute error values of 0.0780 mm, 0.0726 mm, and 0.0555 mm, respectively. The resulting 3D geometric shape and the 3D shape error distribution are shown in Fig. 8. This quantitative assessment provides a convincing proof for that our method can achieve high-precision 3D shape measurement.

![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/3ccaf945-226f-4600-a7ea-adb8706e476b)

**Fig. 7.** A pair of standard spheres.


![image](https://github.com/Feibao77/APR-Net---AbsolutePhaseRetrievalNet/assets/117697608/bede39ad-4d45-4404-bae0-7b9cc5d91b35)

**Fig. 8.** Measurement results of a pair of standard spheres by our method. (a) 3D results. (b) 3D error distribution.


